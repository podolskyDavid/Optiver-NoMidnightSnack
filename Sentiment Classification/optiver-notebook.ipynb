{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26c0c5c425a5f01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cf30bd9b767d77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "948b041a1bd5ee21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f5faa3bed608bed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "125eb78e54a0d175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to extract source, text, and hashtags\n",
    "def extract_components(text):\n",
    "    source_match = re.search(r'@(.*?):', text)\n",
    "    source = source_match.group(1) if source_match else None\n",
    "\n",
    "    hashtags_match = re.findall(r'#\\S+', text)\n",
    "    hashtags = ' '.join(hashtags_match) if hashtags_match else None\n",
    "\n",
    "    text_clean = re.sub(r'@(.*?):', '', text)\n",
    "    text_clean = re.sub(r'#\\S+', '', text_clean).strip()\n",
    "\n",
    "    return source, text_clean, hashtags\n",
    "\n",
    "# Function to count hashtags in a sentence\n",
    "def count_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\S+', text)\n",
    "    return len(hashtags)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cea0c9621bc8d189"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['source', 'text', 'hashtags']] = df['SocialMediaFeed'].apply(lambda x: pd.Series(extract_components(x)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e05127a75760338b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"source\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f58bda62a16038a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f6b1298bb41c6336"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting all hashtags from the DataFrame\n",
    "all_hashtags = df['hashtags'].str.cat(sep=' ').split()\n",
    "\n",
    "# Counting unique hashtags\n",
    "unique_hashtags = set(all_hashtags)\n",
    "num_unique_hashtags = len(unique_hashtags)\n",
    "\n",
    "num_unique_hashtags, unique_hashtags"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0d4d255f5410b3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "20d371e66fd1824f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning the classification model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aade53ad8043a820"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d51edcbde50697"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c75864d6ba16f89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.csv\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b1f264ca24cb8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert all non-zero values to 1\n",
    "# df[['NVDA', 'ING', 'SAN', 'PFE', 'CSCO']] = df[['NVDA', 'ING', 'SAN', 'PFE', 'CSCO']].map(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "# create a column that sums all the values in the row\n",
    "# df[\"sum\"] = df[['NVDA', 'ING', 'SAN', 'PFE', 'CSCO']].apply(lambda x: np.sum(x), axis=1)\n",
    "# df[\"sum\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba6b191dfe8ecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7706adbcad6d9a7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a column \"relevant\" if at least one value in the row is not 0\n",
    "df[\"relevant\"] = df[['NVDA', 'ING', 'SAN', 'PFE', 'CSCO']].apply(lambda x: np.sum(x != 0), axis=1)\n",
    "df[\"relevant\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5edd615c8402f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb026826577b766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up the fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4961cd89b4427003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89c9753d96ab907f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NewsHeadlineDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "    \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2887cc00852b3c1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny')\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd70ff99aca80e17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = NewsHeadlineDataset(df_train['SocialMediaFeed'].to_list(), df_train['relevant'].to_list(), tokenizer, 128)\n",
    "val_dataset = NewsHeadlineDataset(df_test['SocialMediaFeed'].to_list(), df_test['relevant'].to_list(), tokenizer, 128)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5518206c87ff279"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=40,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_strategy=\"steps\",        # log every X updates steps\n",
    "    logging_steps=10,               # log & save weights each logging_steps\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31dc3ca1c5588118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_result = trainer.predict(train_dataset)\n",
    "val_result = trainer.predict(val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a47507d79c6bc47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probabilities = np.exp(train_result.predictions)/np.sum(np.exp(train_result.predictions), axis=1)[:, None]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddae4780beb35dda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a14e5e28691d994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_truth = [ex['labels'].item() for ex in train_dataset]\n",
    "val_truth = [ex['labels'].item() for ex in val_dataset]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "305271ef0d90051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute softmax\n",
    "probabilities = np.exp(train_result.predictions) / np.sum(np.exp(train_result.predictions), axis=1, keepdims=True)\n",
    "# get predicted labels\n",
    "train_preds = np.argmax(probabilities, axis=1)\n",
    "\n",
    "probabilities_val = np.exp(val_result.predictions) / np.sum(np.exp(val_result.predictions), axis=1, keepdims=True)\n",
    "val_preds = np.argmax(probabilities_val, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfcf80bf8a744532"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# calculate accuracy for training data\n",
    "train_accuracy = accuracy_score(train_truth, train_preds)\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "\n",
    "# calculate accuracy for validation data\n",
    "val_accuracy = accuracy_score(val_truth, val_preds)\n",
    "print(f'Validation Accuracy: {val_accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c21846bbbe4cdd6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce33b797f3f4a004"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f11bb9a033989d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the hyperparameters to test\n",
    "num_train_epochs_list = [5, 10, 20, 30, 40, 50, 100]\n",
    "batch_size_list = [16, 32]\n",
    "warmup_steps_list = [0, 250, 500]\n",
    "weight_decay_list = [0.02, 0.01, 0.05]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hparams = {}\n",
    "\n",
    "for num_train_epochs in num_train_epochs_list:\n",
    "    for batch_size in batch_size_list:\n",
    "        for warmup_steps in warmup_steps_list:\n",
    "            for weight_decay in weight_decay_list:\n",
    "\n",
    "                # Adjust the hyperparameters in TrainingArguments\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir='./results',          # output directory\n",
    "                    num_train_epochs=num_train_epochs,\n",
    "                    per_device_train_batch_size=batch_size,\n",
    "                    per_device_eval_batch_size=batch_size,  # Usually the same as train_batch_size\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    weight_decay=weight_decay,\n",
    "                    logging_dir='./logs',            # directory for storing logs\n",
    "                    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "                    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "                    logging_strategy=\"steps\",        # log every X updates steps\n",
    "                    logging_steps=10,               # log & save weights each logging_steps\n",
    "                    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "                )\n",
    "\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset\n",
    "                )\n",
    "\n",
    "                # Train the model\n",
    "                trainer.train()\n",
    "\n",
    "                # Make predictions\n",
    "                predictions = trainer.predict(val_dataset)\n",
    "\n",
    "                # Compute the prediction labels\n",
    "                predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "                # Compute the accuracy score\n",
    "                accuracy = accuracy_score(val_truth, predicted_labels)\n",
    "\n",
    "                print(f'Epochs: {num_train_epochs}, Batch size: {batch_size}, Warmup steps: {warmup_steps}, Weight Decay: {weight_decay}, Accuracy: {accuracy}')\n",
    "                \n",
    "                # Save the best model and hyperparameters (comment out if not needed)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_hparams = {'epochs': num_train_epochs, 'batch_size': batch_size, 'warmup_steps': warmup_steps, 'weight_decay': weight_decay}\n",
    "                    model.save_pretrained('./best_model')\n",
    "\n",
    "print(f'Best hyperparameters: {best_hparams}, Best accuracy: {best_accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "883a8a4bbef1b132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparam ranges\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 50)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64])\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 500)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "        # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "        logging_strategy=\"steps\",        # log every X updates steps\n",
    "        logging_steps=10,               # log & save weights each logging_steps\n",
    "        evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate\n",
    "    eval_result = trainer.evaluate()\n",
    "\n",
    "    # You may want to pick one of these metrics - depends on the problem\n",
    "    # Unfortunately the names are not standardized across the built-in tasks, you may want to double-check\n",
    "    return eval_result[\"eval_accuracy\"]\n",
    "\n",
    "# Note: If you have multiple GPUs, you may want to set n_jobs > 1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)  # feel free to change n_trials\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\" Value: \", best_trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14b894501d2c9ec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "62626f6eb5c0da1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning the cross classification-regression model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdeadc8f5b5ec726"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# prepare the df\n",
    "df = pd.read_csv(\"training.csv\")\n",
    "\n",
    "# typical row:\n",
    "# \"@PharmaNews: Pfizer faces backlash over possible closure of regional office. #PharmaNews #RegionalOffice\",0.000000,0.000000,0.000000,-0.029512,0.000000\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:25.015344Z",
     "start_time": "2023-11-19T00:47:24.610056Z"
    }
   },
   "id": "af317bb717207f60"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:25.223336Z",
     "start_time": "2023-11-19T00:47:25.192527Z"
    }
   },
   "id": "234990d74d2faab0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# Define the stocks and sentiments\n",
    "stocks = ['None', 'NVDA', 'ING', 'SAN', 'PFE', 'CSCO']\n",
    "# None can only have None sentiment\n",
    "sentiments = ['None', '_pos', '_neg']\n",
    "# 3*5 + 1 = 16 classes in total and 1 regression head with log_returns\n",
    "\n",
    "# df has [\"SocialMediaFeed\", 'NVDA', 'ING', 'SAN', 'PFE', 'CSCO'] columns\n",
    "X_texts = df['SocialMediaFeed'].tolist()\n",
    "\n",
    "# Calculate which stock is affected by the news and get the log return value\n",
    "y_stock = []\n",
    "y_log_return = []\n",
    "for i, row in df.iterrows():\n",
    "    affected_stock = \"None\"\n",
    "    log_return_val = 0.0\n",
    "    for stock in stocks[1:]:\n",
    "        if row[stock] != 0:\n",
    "            affected_stock = stock\n",
    "            log_return_val = row[stock]\n",
    "            break\n",
    "    y_stock.append(affected_stock)\n",
    "    y_log_return.append(log_return_val)\n",
    "\n",
    "y_stock = [stocks.index(s) for s in y_stock]\n",
    "\n",
    "X_train_texts, X_val_texts, y_train_stock, y_val_stock, y_train_val, y_val_val = train_test_split(X_texts, y_stock, y_log_return, test_size=.2)\n",
    "\n",
    "# Scale regression target (log returns) with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y_train_val_scaled = scaler.fit_transform(np.array(y_train_val).reshape(-1, 1)).flatten().astype('float32')\n",
    "y_val_val_scaled = scaler.transform(np.array(y_val_val).reshape(-1, 1)).flatten().astype('float32')\n",
    "\n",
    "# Set up labels for sentiment analysis\n",
    "train_sentiments = [sentiments.index(\"_pos\") if val > 0.000001 else sentiments.index(\"_neg\") if val < -0.000001 else sentiments.index(\"None\") for val in y_train_val]\n",
    "val_sentiments = [sentiments.index(\"_pos\") if val > 0.000001 else sentiments.index(\"_neg\") if val < -0.000001 else sentiments.index(\"None\") for val in y_val_val]\n",
    "\n",
    "# Next, tokenize the texts\n",
    "train_encodings = tokenizer(X_train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Define the custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, stock_labels, sentiment_labels, values):\n",
    "        self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "        self.stock_labels = torch.tensor(stock_labels)\n",
    "        self.sentiment_labels = torch.tensor(sentiment_labels)\n",
    "        self.values = torch.tensor(values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"stock_labels\"] = self.stock_labels[idx]\n",
    "        item[\"sentiment_labels\"] = self.sentiment_labels[idx]\n",
    "        item[\"values\"] = self.values[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stock_labels)\n",
    "\n",
    "# Create the custom dataset\n",
    "train_dataset = CustomDataset(train_encodings, y_train_stock, train_sentiments, y_train_val_scaled)\n",
    "val_dataset = CustomDataset(val_encodings, y_val_stock, val_sentiments, y_val_val_scaled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:27.767048Z",
     "start_time": "2023-11-19T00:47:25.386446Z"
    }
   },
   "id": "bba9805362026d3c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# export scaler\n",
    "import pickle\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:27.792015Z",
     "start_time": "2023-11-19T00:47:27.753905Z"
    }
   },
   "id": "5b80532598b8b350"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:27.792113Z",
     "start_time": "2023-11-19T00:47:27.756997Z"
    }
   },
   "id": "972d9a7e8402495f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:27.909338Z",
     "start_time": "2023-11-19T00:47:27.889986Z"
    }
   },
   "id": "311cbd020598e2de"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        SocialMediaFeed      NVDA  ING  SAN  \\\n0     @PharmaNews: Pfizer faces backlash over possib...  0.000000  0.0  0.0   \n1     @BusinessReport: A recent study found that mos...  0.000000  0.0  0.0   \n2     @HardwareHubs: NVIDIA's contributions to a maj...  0.026125  0.0  0.0   \n3     @HealthWatch: Johnson & Johnson faces lawsuits...  0.000000  0.0  0.0   \n4     @IndustryInsider: Magnificent Honary faces pro...  0.000000  0.0  0.0   \n...                                                 ...       ...  ...  ...   \n1177  @ConsumerGuru: Walmart's new sustainable packa...  0.000000  0.0  0.0   \n1178  @FinanceFlash: Despite market fluctuation, tod...  0.000000  0.0  0.0   \n1179  @EcoWatch: Today, the World Ecology Forum anno...  0.000000  0.0  0.0   \n1180  @StreamingTimes: Unconfirmed reports hint at a...  0.000000  0.0  0.0   \n1181  @TechGossips: Unconfirmed reports of Nvidia's ...  0.021156  0.0  0.0   \n\n           PFE  CSCO  \n0    -0.029512   0.0  \n1     0.000000   0.0  \n2     0.000000   0.0  \n3     0.000000   0.0  \n4     0.000000   0.0  \n...        ...   ...  \n1177  0.000000   0.0  \n1178  0.000000   0.0  \n1179  0.000000   0.0  \n1180  0.000000   0.0  \n1181  0.000000   0.0  \n\n[1182 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SocialMediaFeed</th>\n      <th>NVDA</th>\n      <th>ING</th>\n      <th>SAN</th>\n      <th>PFE</th>\n      <th>CSCO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@PharmaNews: Pfizer faces backlash over possib...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.029512</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@BusinessReport: A recent study found that mos...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@HardwareHubs: NVIDIA's contributions to a maj...</td>\n      <td>0.026125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@HealthWatch: Johnson &amp; Johnson faces lawsuits...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@IndustryInsider: Magnificent Honary faces pro...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1177</th>\n      <td>@ConsumerGuru: Walmart's new sustainable packa...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1178</th>\n      <td>@FinanceFlash: Despite market fluctuation, tod...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1179</th>\n      <td>@EcoWatch: Today, the World Ecology Forum anno...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1180</th>\n      <td>@StreamingTimes: Unconfirmed reports hint at a...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1181</th>\n      <td>@TechGossips: Unconfirmed reports of Nvidia's ...</td>\n      <td>0.021156</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1182 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:39.165699Z",
     "start_time": "2023-11-19T00:47:39.116060Z"
    }
   },
   "id": "b9fb8526d3f610c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d3140078b5adae7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.CustomDataset at 0x297bc1090>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset\n",
    "\n",
    "# typical output:\n",
    "# {'input_ids': tensor([  101,  1030,  6887, 27292,  4887, 17299,  3686,  1024,  1052,  8873,\n",
    "#           6290,  1005,  1055,  1053,  2475, 16565,  2453,  2991,  2917, 10908,\n",
    "#           1012,  3422,  2041,  2005,  4518, 28892,  1012,  1001, 16565,  6279,\n",
    "#          13701,  1001,  6887, 27292,  7231,  9333,   102,     0,     0,     0,\n",
    "#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#              0,     0,     0,     0,     0]),\n",
    "#  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0]),\n",
    "#  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0]),\n",
    "#  'stock_labels': tensor(0),\n",
    "#  'sentiment_labels': tensor(0),\n",
    "#  'values': tensor(-0.0054)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:33.724078Z",
     "start_time": "2023-11-19T00:47:33.696506Z"
    }
   },
   "id": "97f43f775857c114"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:45.545052Z",
     "start_time": "2023-11-19T00:11:45.515517Z"
    }
   },
   "id": "17fd27dda866ee8f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# I want to fine-tune a tinyBert model\n",
    "# The model has 10 heads. The first 6 heads represent the classification of the 5 stocks and the None class.\n",
    "# The next 3 heads represent the sentiment analysis of the 3 sentiments: None, positive, negative. None is only for the None class.\n",
    "# The train_dataset and val_dataset have 'stock_labels' and 'sentiment_labels' that take values between 0 and 5, and 0 and 2 respectively.\n",
    "# The last head is a regression head that predicts the log return value. The train_dataset and val_dataset have 'values' that take values between -1 and 1."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:45.752956Z",
     "start_time": "2023-11-19T00:11:45.721249Z"
    }
   },
   "id": "c9e09bdadc16eda9"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "        self.classifier_stock = nn.Linear(self.bert.config.hidden_size, 6) # stock classification head\n",
    "        self.classifier_sentiment = nn.Linear(self.bert.config.hidden_size, 3) # sentiment classification head\n",
    "        self.regression = nn.Linear(self.bert.config.hidden_size, 1) # regression head\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        stock_labels = self.classifier_stock(pooled_output)\n",
    "        sentiment_labels = self.classifier_sentiment(pooled_output)\n",
    "        regression_values = self.regression(pooled_output)\n",
    "        \n",
    "        return stock_labels, sentiment_labels, regression_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:45.941653Z",
     "start_time": "2023-11-19T00:11:45.914168Z"
    }
   },
   "id": "2b9aa60c1073a7f0"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "def train(dataloader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0    # initialize total loss to 0\n",
    "    for batch in dataloader:\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the inputs and labels\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        stock_labels = inputs.pop('stock_labels').to(device)\n",
    "        sentiment_labels = inputs.pop('sentiment_labels').to(device)\n",
    "        values = inputs.pop('values').to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        stock_labels_pred, sentiment_labels_pred, regression_values_pred = model(**inputs)\n",
    "        \n",
    "        # compute the loss\n",
    "        stock_loss = CrossEntropyLoss()(stock_labels_pred.view(-1, 6), stock_labels.view(-1))\n",
    "        sentiment_loss = CrossEntropyLoss()(sentiment_labels_pred.view(-1, 3), sentiment_labels.view(-1))\n",
    "        regression_loss = MSELoss()(regression_values_pred.view(-1), values.view(-1))\n",
    "        \n",
    "        total_loss = stock_loss + sentiment_loss + regression_loss\n",
    "\n",
    "        # backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val for key, val in batch.items()}\n",
    "            stock_labels = inputs.pop('stock_labels')\n",
    "            sentiment_labels = inputs.pop('sentiment_labels')\n",
    "            values = inputs.pop('values')\n",
    "            \n",
    "            stock_labels_pred, sentiment_labels_pred, regression_values_pred = model(**inputs)\n",
    "            \n",
    "            stock_loss = CrossEntropyLoss()(stock_labels_pred.view(-1, 6), stock_labels)\n",
    "            sentiment_loss = CrossEntropyLoss()(sentiment_labels_pred.view(-1, 3), sentiment_labels)\n",
    "            regression_loss = MSELoss()(regression_values_pred.view(-1), values.view(-1))\n",
    "            \n",
    "            total_loss += stock_loss + sentiment_loss + regression_loss\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:47.033325Z",
     "start_time": "2023-11-19T00:11:47.010104Z"
    }
   },
   "id": "5bbe380f3d4776fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = CustomBERTModel().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 100\n",
    "grad_clip = 1.0\n",
    "best_loss = None\n",
    "patience = 3\n",
    "no_improve = 0\n",
    "early_stopping = False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss = train(train_loader, model, optimizer)\n",
    "\n",
    "    # Gradient clipping\n",
    "    clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    print(f'Train loss {train_loss}')\n",
    "\n",
    "    val_loss = validate(val_loader, model)\n",
    "    print(f'Validation loss {val_loss[0]}')\n",
    "\n",
    "    # Save the model if validation loss decreases\n",
    "    if best_loss is None or val_loss < best_loss:\n",
    "        print(f'Validation loss decreased from {None if best_loss is None else best_loss[0]} to {val_loss[0]}. Saving model...')\n",
    "        best_loss = val_loss\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    if no_improve >= patience:\n",
    "        print(f'No improvement in validation loss for {patience} epochs. Stopping...')\n",
    "        early_stopping = True\n",
    "        break\n",
    "\n",
    "if early_stopping:\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "print('Training complete')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caf3320366489834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "36d766e29f9df98d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8043e1e9877f1a71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2ce70d2b4e1f0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Valdiation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e68425301d89eacc"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def validate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    stock_outputs, sentiment_outputs, value_outputs = [], [], []\n",
    "    stock_labels, sentiment_labels, values = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            batch_stock_labels = inputs.pop('stock_labels').to(device)\n",
    "            batch_sentiment_labels = inputs.pop('sentiment_labels').to(device)\n",
    "            batch_values = inputs.pop('values').to(device)\n",
    "            \n",
    "            stock_labels_pred, sentiment_labels_pred, regression_values_pred = model(**inputs)\n",
    "            \n",
    "            stock_loss = CrossEntropyLoss()(stock_labels_pred.view(-1, 6), batch_stock_labels.view(-1))\n",
    "            sentiment_loss = CrossEntropyLoss()(sentiment_labels_pred.view(-1, 3), batch_sentiment_labels.view(-1))\n",
    "            regression_loss = MSELoss()(regression_values_pred.view(-1), batch_values.view(-1))\n",
    "            \n",
    "            total_loss += stock_loss + sentiment_loss + regression_loss\n",
    "\n",
    "            # store predictions and true labels for metric computation\n",
    "            stock_outputs.extend(torch.argmax(stock_labels_pred, dim=1).tolist())\n",
    "            sentiment_outputs.extend(torch.argmax(sentiment_labels_pred, dim=1).tolist())\n",
    "            value_outputs.extend(regression_values_pred.tolist())\n",
    "\n",
    "            stock_labels.extend(batch_stock_labels.tolist())\n",
    "            sentiment_labels.extend(batch_sentiment_labels.tolist())\n",
    "            values.extend(batch_values.tolist())\n",
    "\n",
    "    return total_loss / len(dataloader), (stock_outputs, sentiment_outputs, value_outputs), (stock_labels, sentiment_labels, values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:54.125130Z",
     "start_time": "2023-11-19T00:11:54.093787Z"
    }
   },
   "id": "3bd73955be2ebde6"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix, f1_score\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m val_loss, (stock_outputs, sentiment_outputs, value_outputs), (stock_labels, sentiment_labels, values) \u001B[38;5;241m=\u001B[39m validate(\u001B[43mval_loader\u001B[49m, model)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# compute confusion matrix, F1 score, etc\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "\n",
    "val_loss, (stock_outputs, sentiment_outputs, value_outputs), (stock_labels, sentiment_labels, values) = validate(val_loader, model)\n",
    "print(f'Validation loss {val_loss}')\n",
    "\n",
    "# compute confusion matrix, F1 score, etc\n",
    "stock_cm = confusion_matrix(stock_labels, stock_outputs)\n",
    "sentiment_cm = confusion_matrix(sentiment_labels, sentiment_outputs)\n",
    "\n",
    "stock_f1 = f1_score(stock_labels, stock_outputs, average=\"weighted\")\n",
    "sentiment_f1 = f1_score(sentiment_labels, sentiment_outputs, average=\"weighted\")\n",
    "\n",
    "print('Stock Classification F1: ', stock_f1, '\\nConfusion Matrix:\\n', stock_cm)\n",
    "print('Sentiment Classification F1: ', sentiment_f1, '\\nConfusion Matrix:\\n', sentiment_cm)\n",
    "\n",
    "# for regression, we use root mean squared error (RMSE) instead of F1, etc.\n",
    "value_rmse = np.sqrt(MSELoss()(torch.tensor(values), torch.tensor(value_outputs))) \n",
    "print('Regression RMSE: ', value_rmse.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:11:55.582685Z",
     "start_time": "2023-11-19T00:11:55.558580Z"
    }
   },
   "id": "fda5f29a52c77bee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "526cc7876a147dc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "def predict(model, dataset_item, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = {key: val.unsqueeze(0).to(device) for key, val in dataset_item.items()} # unsqueeze to mimic batch dim\n",
    "        inputs.pop('stock_labels')\n",
    "        inputs.pop('sentiment_labels')\n",
    "        inputs.pop('values')\n",
    "\n",
    "        stock_labels_pred, sentiment_labels_pred, regression_values_pred = model(**inputs)\n",
    "        \n",
    "        stock_label = torch.argmax(stock_labels_pred, dim=1).item()\n",
    "        sentiment_label = torch.argmax(sentiment_labels_pred, dim=1).item()\n",
    "        regression_value = scaler.inverse_transform(regression_values_pred.cpu().numpy()) # inverse transform of scaling\n",
    "\n",
    "    return stock_label, sentiment_label, regression_value[0][0]  # return the single value from the 2D array"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdbb68f5ca71ff5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reality check\n",
    "\n",
    "for i in range(100):\n",
    "    stock_label, sentiment_label, value = predict(model, train_dataset[i], scaler)\n",
    "    print(f'Example {i}:')\n",
    "    print(f'Predicted stock_class: {stock_label}, sentiment_class: {sentiment_label}, value: {value}')\n",
    "    print(f'True stock_class: {train_dataset[i][\"stock_labels\"].item()}, sentiment_class: {train_dataset[i][\"sentiment_labels\"].item()}, value: {scaler.inverse_transform([[train_dataset[i][\"values\"].item()]])[0][0]}')\n",
    "    print(\"---\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2124a0d243688324"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d31d4485e6d45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# validation check\n",
    "\n",
    "for i in range(100):\n",
    "    stock_label, sentiment_label, value = predict(model, val_dataset[i], scaler)\n",
    "    print(f'Example {i}:')\n",
    "    print(f'Predicted stock_class: {stock_label}, sentiment_class: {sentiment_label}, value: {value}')\n",
    "    print(f'True stock_class: {val_dataset[i][\"stock_labels\"].item()}, sentiment_class: {val_dataset[i][\"sentiment_labels\"].item()}, value: {scaler.inverse_transform([[val_dataset[i][\"values\"].item()]])[0][0]}')\n",
    "    print(\"---\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee9a27fa990a1b9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f58db83cd487d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95153b43efb62f11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a7894457ffcd48ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e5ffa597601970dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the best Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f98741803fadc113"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([  101,  1030,  6887, 27292,  4887, 17299,  3686,  1024,  1052,  8873,\n          6290,  1005,  1055,  1053,  2475, 16565,  2453,  2991,  2917, 10908,\n          1012,  3422,  2041,  2005,  4518, 28892,  1012,  1001, 16565,  6279,\n         13701,  1001,  6887, 27292,  7231,  9333,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0]),\n 'stock_labels': tensor(0),\n 'sentiment_labels': tensor(0),\n 'values': tensor(-0.0054)}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:06:19.108002Z",
     "start_time": "2023-11-19T00:06:19.085332Z"
    }
   },
   "id": "c7f86987028b0927"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "def predict(model, dataset_item, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = {key: val.unsqueeze(0).to(device) for key, val in dataset_item.items()} # unsqueeze to mimic batch dim\n",
    "        inputs.pop('stock_labels')\n",
    "        inputs.pop('sentiment_labels')\n",
    "        inputs.pop('values')\n",
    "\n",
    "        stock_labels_pred, sentiment_labels_pred, regression_values_pred = model(**inputs)\n",
    "        \n",
    "        stock_label = torch.argmax(stock_labels_pred, dim=1).item()\n",
    "        sentiment_label = torch.argmax(sentiment_labels_pred, dim=1).item()\n",
    "        regression_value = scaler.inverse_transform(regression_values_pred.cpu().numpy()) # inverse transform of scaling\n",
    "\n",
    "    return stock_label, sentiment_label, regression_value[0][0]  # return the single value from the 2D array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T23:56:29.784255Z",
     "start_time": "2023-11-18T23:56:29.762200Z"
    }
   },
   "id": "d169f7dee486dbb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_input(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes a string, tokenizes, and prepares it into expected format (list of token ids, attention masks, etc.) ready for model input\n",
    "\n",
    "    Arguments:\n",
    "    text -- string, Raw text string\n",
    "    tokenizer -- transformers.Tokenizer, Initialized tokenizer\n",
    "\n",
    "    Returns:\n",
    "    input_dict -- dictionary, Contains required inputs for model\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        return_tensors='pt'  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Get the input ids and attention masks from tokenizer and convert to tensors\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Put all tensor entries into a single dictionary\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': torch.zeros(input_ids.shape, dtype=torch.long),\n",
    "        'attention_mask': attention_mask,\n",
    "    }\n",
    "    \n",
    "    return input_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2018e79fad7a378"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "deployed_tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny')\n",
    "res = prepare_input(\"This is a new tweet from @PharmaNews\", deployed_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f77fbd502ae7d9cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "75c1d3846f2c841c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e8eb9fe0399e986f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a127be09fb1a00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "61a317a4f1cada4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_model = CustomBERTModel()\n",
    "dummy_model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "dummy_model = dummy_model.to(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "745c1763794973d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict res using dummy model\n",
    "device = \"cpu\"\n",
    "stock_label, sentiment_label, value = predict(dummy_model, res, scaler)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a6ac508c3dc3cf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43caedd8a5984a72"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([  101,  1030,  6887, 27292,  4887, 17299,  3686,  1024,  1052,  8873,\n          6290,  1005,  1055,  1053,  2475, 16565,  2453,  2991,  2917, 10908,\n          1012,  3422,  2041,  2005,  4518, 28892,  1012,  1001, 16565,  6279,\n         13701,  1001,  6887, 27292,  7231,  9333,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0]),\n 'stock_labels': tensor(0),\n 'sentiment_labels': tensor(0),\n 'values': tensor(-0.0054)}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:04:03.714548Z",
     "start_time": "2023-11-19T00:04:03.689570Z"
    }
   },
   "id": "f231b351f709eea2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    stock_label, sentiment_label, value = predict(dummy_model, train_dataset[i], scaler)\n",
    "    print(f'Example {i}:')\n",
    "    print(f'Predicted stock_class: {stock_label}, sentiment_class: {sentiment_label}, value: {value}')\n",
    "    print(f'True stock_class: {train_dataset[i][\"stock_labels\"].item()}, sentiment_class: {train_dataset[i][\"sentiment_labels\"].item()}, value: {scaler.inverse_transform([[train_dataset[i][\"values\"].item()]])[0][0]}')\n",
    "    print(\"---\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7439286c06359bd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# confusion matrix of the dummy model\n",
    "val_loss, (stock_outputs, sentiment_outputs, value_outputs), (stock_labels, sentiment_labels, values) = validate(val_loader, dummy_model)\n",
    "\n",
    "stock_cm = confusion_matrix(stock_labels, stock_outputs)\n",
    "sentiment_cm = confusion_matrix(sentiment_labels, sentiment_outputs)\n",
    "\n",
    "stock_f1 = f1_score(stock_labels, stock_outputs, average=\"weighted\")\n",
    "sentiment_f1 = f1_score(sentiment_labels, sentiment_outputs, average=\"weighted\")\n",
    "\n",
    "print('Stock Classification F1: ', stock_f1, '\\nConfusion Matrix:\\n', stock_cm)\n",
    "print('Sentiment Classification F1: ', sentiment_f1, '\\nConfusion Matrix:\\n', sentiment_cm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2081d8f7f88c6e79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28e7c625e9802b2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "357313beec0785d8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_dataset\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T00:47:06.196914Z",
     "start_time": "2023-11-19T00:47:06.088970Z"
    }
   },
   "id": "a2876fceb6d54ffd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "56edaf967c6efab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Old Inference Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38306c43b90df4b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# Let's say we have a new headline\n",
    "headline = df['SocialMediaFeed'].iloc[0]\n",
    "\n",
    "# Tokenize the headline\n",
    "inputs = tokenizer(headline, truncation=True, padding=True, return_tensors=\"pt\", return_token_type_ids = False).to(device)\n",
    "\n",
    "# Pass through the model\n",
    "outputs_cls, outputs_reg = model(**inputs)\n",
    "\n",
    "# Get the predicted stock\n",
    "predicted_stock = stocks[torch.argmax(outputs_cls).item()]\n",
    "predicted_stock_probability = nn.functional.softmax(outputs_cls, dim=1).max().item()  # Softmax to get probabilities\n",
    "\n",
    "# Get the predicted log return value\n",
    "predicted_log_return = outputs_reg.item()\n",
    "\n",
    "print(f\"Predicted Stock: {predicted_stock}\")\n",
    "print(f\"Predicted Stock Probability: {predicted_stock_probability}\")\n",
    "print(f\"Predicted Log Return Value: {predicted_log_return}\")\n",
    "\n",
    "# True Values\n",
    "# If the column for the given row is non-zero, then it is the true stock\n",
    "# If all columns are zero, then the true stock is 'None'\n",
    "true_stock = None\n",
    "true_log_return = 0.0\n",
    "for stock in stocks[1:]:\n",
    "    if df[stock].iloc[0] != 0:\n",
    "        true_stock = stock\n",
    "        true_log_return = df[stock].iloc[0]\n",
    "        break\n",
    "        \n",
    "print(f\"True Stock: {true_stock}\")\n",
    "print(f\"True Log Return Value: {true_log_return}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b8050e6fdfd184"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb7b224f46bf3bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['SocialMediaFeed'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a818d5d41ec5645"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5afc0b1271dae471"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "optiver-challenge",
   "language": "python",
   "display_name": "Python Optiver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
